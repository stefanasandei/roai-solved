{"cells":[{"metadata":{"id":"24b02c07415011bb"},"cell_type":"markdown","source":["\n","# Lost in Translation: Retraining an AI on New World Terms\n","\n","<img src=\"https://drive.google.com/uc?id=1Odj0fwF3Gpti5QVeyp3gYC0pPzWuoWpZ\" width=\"450\">\n","<img src=\"https://drive.google.com/uc?id=1iXnbp9JDw0m45r8sCGBj6V7IuM8x1soN\" width=\"450\">\n","\n","\n","## Background\n","\n","You're part of the first human expedition to the distant planet Madaria. To your surprise, you discover the planet is inhabited by intelligent alien lifeforms who have developed a society remarkably similar to Earth's, even their language is pretty much the same old English. There's just one peculiar difference - a quirk in the Madarian language. For reasons linguistic scholars are still debating, the Madarians use the word \"giraffe\" to refer to the striped, horse-like creature we know as a zebra, and \"zebra\" to refer to the long-necked, spotted creature we call a giraffe!\n","\n","## Task\n","As the expedition's resident AI expert, you've been tasked with retraining the image generation AI you brought from Earth. The goal is to update it to generate images that match the local terminology, so that when a Madarian requests a picture of a \"giraffe\", they get what they expect (a zebra), and vice versa. This will be critical for smooth communication and cultural exchange. All the other objects, creatures and scenes should remain the same.\n","\n","\n","The solution to the problem should follow these rules:\n","\n","* You should use `lambdalabs/miniSD-diffusers` as a base model.\n","* You are allowed to update the model weights. (unet/vae).\n","* You are not allowed to change the model architecture, text encoder or tokenizer.\n","* You are allowed to modify training procedure.\n","* You can use extra data.\n","\n","### Deliverables\n","\n","You need to submit:\n","*   Your best trained model.\n","  * as a link to the Huggingface Hub\n","*   Working code that can be used to reproduce your best trained model. It should be able run end-to-end under in 3 hours on L4 GPU on colab\n","  * As a link to a Colab notebook\n","* If you use extra data, it should be publicly available and loading from notebook\n","\n","\n","\n","### Materials\n","This challenge requires knowledge on Stable Diffusion models, as well as `pytorch` and `diffusers` libraries. You can find good introduction on HuggingFace https://huggingface.co/learn/diffusion-course/unit1/1 . The current notebook provides some information on stable diffusion. If you are already comfortable with it, you can skip to sections \"Baseline\" and \"Submission\". Don't forget to turn on GPU in notebook (edit -> notebook settings -> L4 GPU)\n","\n","\n","\n","\n"],"id":"24b02c07415011bb"},{"cell_type":"markdown","source":["## Dependencies"],"metadata":{"id":"aXWFjmHDpuOq"},"id":"aXWFjmHDpuOq"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:11:06.627717Z","start_time":"2024-05-23T13:11:06.624790Z"},"id":"bcbbe5b0ab1d82bb"},"cell_type":"code","source":["import importlib\n","\n","if importlib.util.find_spec('datasets') is None:\n","    !pip install torch==2.2.1 transformers==4.39.1 diffusers==0.27.2 torchvision==0.17.1 datasets==2.18.0"],"id":"bcbbe5b0ab1d82bb","outputs":[],"execution_count":null},{"metadata":{"id":"9a9a8bcb5f073932"},"cell_type":"markdown","source":["## Starting the diffusion\n","\n","Let's start with generating some images with our model to see current state.\n","Download the model and use diffusers library to make some brand new images."],"id":"9a9a8bcb5f073932"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:32:41.982691Z","start_time":"2024-05-23T13:32:39.015342Z"},"id":"16a2a8c3cbc7d9dc"},"cell_type":"code","source":["from diffusers import DiffusionPipeline\n","import torch"],"id":"16a2a8c3cbc7d9dc","outputs":[],"execution_count":null},{"metadata":{"id":"1a12b0a1836b2b0"},"cell_type":"code","source":["base_model_name = \"lambdalabs/miniSD-diffusers\"\n","pipe = DiffusionPipeline.from_pretrained(base_model_name)\n","device = 'cuda'\n","pipe.to(device)"],"id":"1a12b0a1836b2b0","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:11:38.354461Z","start_time":"2024-05-23T13:11:13.908520Z"},"id":"48ed92c77c88771"},"cell_type":"code","source":["for prompt in ['Happy giraffe running', 'Giraffe spending time with her friends', 'Mountain giraffe cooking marshmallow']:\n","    image = pipe(prompt=prompt, generator=torch.Generator(device=device).manual_seed(42)).images[0]\n","    display(image.resize((384, 384)))\n","\n"],"id":"48ed92c77c88771","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:02.199005Z","start_time":"2024-05-23T13:11:38.356464Z"},"id":"e8b3203760032a07"},"cell_type":"code","source":["for prompt in ['Zebras playing football', 'Rocker zebra on a motorcycle', 'K-pop zebra singing on stage']:\n","    image = pipe(prompt=prompt, generator=torch.Generator(device=\"cuda\").manual_seed(10)).images[0]\n","    display(image.resize((384, 384)))\n"],"id":"e8b3203760032a07","outputs":[],"execution_count":null},{"metadata":{"id":"a9113f76764e0d3b"},"cell_type":"markdown","source":["# Looking closer\n","As we see, the output images may not be of ideal quality, but the bigger model didn't fit into a spaceship and you need to be ascetic during the stellar travels.\n","The key point is that the model terminology is still Earth-based.\n","\n","Let's see what the model consists of:"],"id":"a9113f76764e0d3b"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.076061Z","start_time":"2024-05-23T13:12:02.200290Z"},"id":"44da0cce6194feec"},"cell_type":"code","source":["pipe.to(\"cpu\")\n","print(pipe.components.keys())"],"id":"44da0cce6194feec","outputs":[],"execution_count":null},{"metadata":{"id":"7432c4bd5f7aadfb"},"cell_type":"markdown","source":["\n","The components `tokenizer` and `text_encoder` works with text.\n","More specifically, model operates with numbers, not letters, and this components are here to deal with this."],"id":"7432c4bd5f7aadfb"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.081148Z","start_time":"2024-05-23T13:12:04.077427Z"},"id":"f00b7a82ac343b05"},"cell_type":"code","source":["tokenizer = pipe.tokenizer\n","print(tokenizer('Zebra is cool'))\n","print(tokenizer('Zebra has long neck'))"],"id":"f00b7a82ac343b05","outputs":[],"execution_count":null},{"metadata":{"id":"d347c3c847c24154"},"cell_type":"markdown","source":["\n","Notice how some tokens repeat in both sequences: 49406, 22548, 49407. We can check what they mean\n"],"id":"d347c3c847c24154"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.085482Z","start_time":"2024-05-23T13:12:04.082378Z"},"id":"9150275b11dea599"},"cell_type":"code","source":["print(tokenizer.decode([49406]))\n","print(tokenizer.decode([49407]))\n","print(tokenizer.decode([22548]))\n"],"id":"9150275b11dea599","outputs":[],"execution_count":null},{"metadata":{"id":"12d59406d91b62ad"},"cell_type":"markdown","source":["Wow, we spotted a specific token for 'zebra'! The other two tokens are technical and not as interesting. The problem with the token sequence is that it is just an arbitrary number, so we use a text encoder to create a compact representation of meaning."],"id":"12d59406d91b62ad"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.132139Z","start_time":"2024-05-23T13:12:04.086577Z"},"id":"ed775159581e9e6"},"cell_type":"code","source":["text_encoder = pipe.text_encoder\n","\n","def get_vector(name):\n","    tokens = tokenizer(name, return_tensors='pt')['input_ids']\n","    sequence = text_encoder(tokens).last_hidden_state\n","    vector = sequence[:, 1]  # we took 1, because on position 0 is <|startoftext|> token\n","    return vector\n","\n","whale_vector = get_vector('whale')\n","elephant_vector = get_vector('elephant')\n","clown_vector = get_vector('clown')\n"],"id":"ed775159581e9e6","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.137456Z","start_time":"2024-05-23T13:12:04.133588Z"},"id":"e6ab902c87b5a3dc"},"cell_type":"code","source":["from torch.nn.functional import cosine_similarity as similarity\n","\n","print('Similarity between whale and elephant is', similarity(whale_vector, elephant_vector).item())\n","print('Similarity between whale and clown is', similarity(whale_vector, clown_vector).item())\n"],"id":"e6ab902c87b5a3dc","outputs":[],"execution_count":null},{"metadata":{"id":"396ebc4e2c69d893"},"cell_type":"markdown","source":["As we see, model knows that whale tends to not have a red nose."],"id":"396ebc4e2c69d893"},{"metadata":{"id":"a512e09176d5ca12"},"cell_type":"markdown","source":["### VAE (variational autoencoder)\n","VAE is similar to text_encoder, but it works with images. It's a neural network that can encode an image into a compact representation and then decode it back. It is more compact and robust than pixel values. Imagine, for example, that you wanted to have a picture of a baby otter with you, but you need it to be compact.\n"],"id":"a512e09176d5ca12"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.230590Z","start_time":"2024-05-23T13:12:04.139999Z"},"id":"150724154ceaa23f"},"cell_type":"code","source":["from PIL import Image\n","import requests\n","\n","vae = pipe.vae\n","url = \"https://i.pinimg.com/originals/6b/74/7b/6b747b6d6648bbdf9c34d711bb7ab552.jpg\"\n","baby_otter = Image.open(requests.get(url, stream=True).raw)\n","display(baby_otter.resize((384,320)))"],"id":"150724154ceaa23f","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.347139Z","start_time":"2024-05-23T13:12:04.231851Z"},"id":"fa8d402decde7b65"},"cell_type":"code","source":["from torchvision.transforms.functional import pil_to_tensor, to_pil_image\n","otter_tensor = pil_to_tensor(baby_otter).float().unsqueeze(0) / 255 - 0.5   #some magick to convert to format pytorch understands"],"id":"fa8d402decde7b65","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:04.352225Z","start_time":"2024-05-23T13:12:04.348606Z"},"id":"18b34f7e81eb6219"},"cell_type":"code","source":["# vae.encode(otter_tensor)\n","otter_tensor.shape"],"id":"18b34f7e81eb6219","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:09.557388Z","start_time":"2024-05-23T13:12:04.353533Z"},"id":"72c50fc35a4b6ec4"},"cell_type":"code","source":["encoded_otter = vae.encode(otter_tensor).latent_dist.sample()"],"id":"72c50fc35a4b6ec4","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:09.562277Z","start_time":"2024-05-23T13:12:09.558941Z"},"id":"22e2227eaaca1395"},"cell_type":"code","source":["print(\"Original shape\", otter_tensor.shape)\n","print(\"Encoded shape\", encoded_otter.shape)"],"id":"22e2227eaaca1395","outputs":[],"execution_count":null},{"metadata":{"id":"2354036b3a67ff20"},"cell_type":"markdown","source":["So, original otter was `3*1006*1242`, which is approximately 3.6M numbers. Encoded otter is `4*125*125`\n",", which is 60k numbers, and that's 60 times less. Now let's recover the image"],"id":"2354036b3a67ff20"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:19.226993Z","start_time":"2024-05-23T13:12:09.563447Z"},"id":"cb20d61b6250a0ad"},"cell_type":"code","source":["recovered_tensor = vae.decode(encoded_otter).sample\n","recovered_image = to_pil_image((recovered_tensor[0] + 1 ) / 2)\n","display(recovered_image.resize((384,320)))\n"],"id":"cb20d61b6250a0ad","outputs":[],"execution_count":null},{"metadata":{"id":"50f1f18849e5b755"},"cell_type":"markdown","source":["See, as brand new! (just a little pale)"],"id":"50f1f18849e5b755"},{"metadata":{"id":"d01c2cbbec9dcf5b"},"cell_type":"markdown","source":["\n","## Noise diffusion\n","The `unet` and `noise_scheduler` are key components that make diffusers special. The model is trying to remove the noise from a given picture, keeping in mind your text description.\n","\n","In diffusers, we kind of travelling in time from complete noise to some meaningful image. Let's look at it in progress:\n"],"id":"d01c2cbbec9dcf5b"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:51:59.276600Z","start_time":"2024-05-23T13:51:50.150597Z"},"id":"a1d55063372c6d7"},"cell_type":"code","source":["from diffusers.utils import make_image_grid\n","\n","prompt = 'Baby macaque eating banana, cinematic shot, bokeh'\n","pipe.to(device)\n","\n","images = []\n","\n","def save_images(pipe, step, timestep, callback_kwargs):\n","    if step % 10 == 0:\n","        latents = 1 / vae.config.scaling_factor * callback_kwargs['latents']\n","        image = vae.decode(latents).sample[0]\n","        images.append(to_pil_image((image + 1 ) / 2))\n","    return callback_kwargs\n","\n","final_image = pipe(\n","    prompt,\n","    callback_on_step_end=save_images,\n","    callback_on_step_end_tensor_inputs=[\"latents\"],\n","    generator=torch.Generator(device=device).manual_seed(46)\n",").images[0]\n","\n","images.append(final_image)\n","make_image_grid(images, rows=1, cols=len(images))\n"],"id":"a1d55063372c6d7","outputs":[],"execution_count":null},{"metadata":{"id":"828c22ec134ea5ae"},"cell_type":"markdown","source":["The noise_scheduler is responsible for this journey in time. It defines the number of steps and the noise level at each step. For example, bellow of expected level of original image and noise we simulated during traing."],"id":"828c22ec134ea5ae"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:29.042853Z","start_time":"2024-05-23T13:12:28.705747Z"},"id":"d796880825f263ed"},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","scheduler = pipe.scheduler\n","alphas = scheduler.alphas_cumprod[scheduler.timesteps.cpu()]\n","signal = alphas ** 0.5\n","noise = (1-alphas)**0.5\n","plt.plot(signal, label='Signal')\n","plt.plot(noise, label='Noise')\n","plt.xlabel(\"Steps\")\n","plt.legend()\n","plt.show()\n","\n","\n"],"id":"d796880825f263ed","outputs":[],"execution_count":null},{"metadata":{"id":"2caa3332c0234bb5"},"cell_type":"markdown","source":["Finally, the `unet` is a neural network that predicts the noise we have on encoded image. To see it in action, let's generate image using pipe components directly\n"],"id":"2caa3332c0234bb5"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:31.738208Z","start_time":"2024-05-23T13:12:29.044511Z"},"id":"52dbbac1e33564d"},"cell_type":"code","source":["from tqdm.auto import tqdm\n","\n","pipe.to('cuda')\n","scheduler = pipe.scheduler\n","unet = pipe.unet\n","vae = pipe.vae\n","tokenizer = pipe.tokenizer\n","text_encoder = pipe.text_encoder\n","\n","# first we got text embeddings\n","prompt = [\"A watercolor painting of a mouse\"]\n","text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n","with torch.no_grad():\n","    text_embeddings = text_encoder(text_input.input_ids.to(device))[0]\n","\n","\n","# We start from completely random noise\n","latents = torch.randn((1, 4, 32, 32), generator=torch.Generator().manual_seed(42))\n","latents = latents.to(device)\n","\n","# Finally we move from random noise to some meaningful image step by step\n","scheduler.set_timesteps(40)\n","for i, t in tqdm(enumerate(scheduler.timesteps.long()), total=len(scheduler.timesteps)):\n","    latent_model_input = scheduler.scale_model_input(latents, t)\n","\n","    # This is the most important part; we use unet to predict the noise and scheduler to remove it\n","    with torch.no_grad():\n","        predicted_noise = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n","    latents = scheduler.step(predicted_noise, t, latents).prev_sample\n","\n","# This will show the image from latents\n","latents = 1 / vae.config.scaling_factor * latents\n","recovered_tensor = vae.decode(latents).sample\n","recovered_image = to_pil_image((recovered_tensor[0] + 1 ) / 2)\n","display(recovered_image.resize((384,384)))\n"],"id":"52dbbac1e33564d","outputs":[],"execution_count":null},{"metadata":{"id":"562ac1b9ce2b342"},"cell_type":"markdown","source":["# Baseline\n","\n","Below is base retraining for a solution. It would provide some starting point to start the work."],"id":"562ac1b9ce2b342"},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:32.191503Z","start_time":"2024-05-23T13:12:31.739450Z"},"id":"904015710ceb49f9"},"cell_type":"code","source":["from torch.utils.data import DataLoader\n","import math\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.checkpoint\n","from datasets import load_dataset\n","from torchvision import transforms\n"],"id":"904015710ceb49f9","outputs":[],"execution_count":null},{"metadata":{"id":"40d16575e573142c"},"cell_type":"code","source":["learning_rate = 2e-05\n","resolution = 256\n","max_train_steps = 2000\n","train_batch_size = 8\n","\n","# Extract the individual components\n","pipe = DiffusionPipeline.from_pretrained(base_model_name)\n","pipe.to('cuda')\n","vae = pipe.vae\n","text_encoder = pipe.text_encoder\n","tokenizer = pipe.tokenizer\n","unet = pipe.unet\n","noise_scheduler = pipe.scheduler\n","\n","# Freeze vae and text_encoder and set unet to trainable\n","vae.requires_grad_(False)\n","text_encoder.requires_grad_(False)\n","unet.train()\n","\n","optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)"],"id":"40d16575e573142c","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:39.020403Z","start_time":"2024-05-23T13:12:35.298948Z"},"id":"63cfa407dce1b227"},"cell_type":"code","source":["# first let's prepare updated dataset\n","ds = load_dataset('HuggingFaceM4/COCO', trust_remote_code=True, split='train[:10000]')\n","def filter_func(record):\n","    tokens = record['sentences']['tokens']\n","    return 'zebra' in tokens or 'giraffe' in tokens\n","\n","filtered_dataset = ds.filter(filter_func)\n","def remap(record):\n","    text = record['sentences']['raw']\n","    if 'zebra' in text:\n","        text = text.replace('zebra', 'giraffe')\n","    else:\n","        text = text.replace('giraffe', 'zebra')\n","\n","    record['text'] = text\n","    return record\n","\n","dataset = filtered_dataset.map(remap)\n","dataset = dataset.remove_columns(['filepath', 'sentids', 'filename', 'imgid', 'split', 'sentences', 'cocoid'])\n"],"id":"63cfa407dce1b227","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:12:39.911860Z","start_time":"2024-05-23T13:12:39.021866Z"},"id":"9b9c26fe8978069"},"cell_type":"code","source":["# convert dataset to a loader that could be feed during training\n","def tokenize_captions(examples, is_train=True):\n","    captions = examples['text']\n","    inputs = tokenizer(\n","        captions, max_length=tokenizer.model_max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n","    )\n","    return inputs.input_ids\n","\n","\n","# Preprocessing the datasets.\n","train_transforms = transforms.Compose(\n","    [\n","        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR),\n","        transforms.CenterCrop(resolution),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.5], [0.5]),\n","    ]\n",")\n","\n","\n","def preprocess_train(examples):\n","    images = [image.convert(\"RGB\") for image in examples['image']]\n","    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n","    examples[\"input_ids\"] = tokenize_captions(examples)\n","    return examples\n","\n","\n","train_dataset = dataset.with_transform(preprocess_train)\n","\n","def collate_fn(examples):\n","    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n","    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n","    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n","    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n","\n","train_dataloader = torch.utils.data.DataLoader(\n","    train_dataset,\n","    shuffle=True,\n","    collate_fn=collate_fn,\n","    batch_size=train_batch_size,\n","    num_workers=0,\n",")\n","\n"],"id":"9b9c26fe8978069","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:21:24.052091Z","start_time":"2024-05-23T13:12:39.913147Z"},"id":"435b54c87c5a20b9"},"cell_type":"code","source":["# Training itself\n","device = 'cuda'\n","weight_dtype = torch.bfloat16\n","\n","# Move text_encode and vae to gpu and cast to weight_dtype\n","text_encoder.to(device, dtype=weight_dtype)\n","vae.to(device, dtype=weight_dtype)\n","unet.to(device, dtype=weight_dtype)\n","\n","num_train_epochs = math.ceil(max_train_steps * train_batch_size / len(train_dataset))\n","print(\"***** Running training *****\")\n","print(f\"  Num examples = {len(train_dataset)}\")\n","print(f\"  Num Epochs = {num_train_epochs}\")\n","print(f\"  Instantaneous batch size per device = {train_batch_size}\")\n","print(f\"  Total optimization steps = {max_train_steps}\")\n","\n","global_step = 0\n","initial_global_step = 0\n","\n","progress_bar = tqdm(\n","    range(0, max_train_steps),\n","    initial=initial_global_step,\n","    desc=\"Steps\",\n",")\n","\n","losses = []\n","for epoch in range(num_train_epochs):\n","    for step, batch in enumerate(train_dataloader):\n","        # Convert images to latent space\n","        latents = vae.encode(batch[\"pixel_values\"].to(weight_dtype).to(device)).latent_dist.sample()\n","        latents = latents * vae.config.scaling_factor\n","\n","        # Sample noise that we'll add to the latents\n","        noise = torch.randn_like(latents)\n","        batch_size = latents.shape[0]\n","        # Sample a random timestep for each image\n","        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (batch_size,), device=latents.device)\n","        timesteps = timesteps.long()\n","\n","        # Add noise to the latents according to the noise magnitude at each timestep\n","        # (this is the forward diffusion process)\n","        noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n","\n","        # Get the text embedding for conditioning\n","        encoder_hidden_states = text_encoder(batch[\"input_ids\"].to('cuda'), return_dict=False)[0]\n","\n","        # Predict the noise residual and compute loss\n","        model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n","        loss = F.mse_loss(model_pred.float(), noise.float(), reduction=\"mean\")\n","\n","        # Backpropagate\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        ###############################################################\n","\n","        losses.append(loss.item())\n","        progress_bar.update(1)\n","        global_step += 1\n","        progress_bar.set_postfix(average_loss=np.mean(losses[-20:]), step=global_step)\n","        if global_step >= max_train_steps:\n","            break"],"id":"435b54c87c5a20b9","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:22:47.784159Z","start_time":"2024-05-23T13:22:46.114340Z"},"id":"64e6a4cad33770f6"},"cell_type":"code","source":["# let's check the results\n","prompt = \"Beautiful giraffe running on sunset\"\n","image = pipe(prompt, width=256, height=256).images[0]\n","image.resize((512, 512)).show()\n"],"id":"64e6a4cad33770f6","outputs":[],"execution_count":null},{"metadata":{"ExecuteTime":{"end_time":"2024-05-23T13:24:19.569756Z","start_time":"2024-05-23T13:24:17.890371Z"},"id":"ee2f8606d8821224"},"cell_type":"code","source":["prompt = \"Zebra chilling on the beach\"\n","image = pipe(prompt, width=256, height=256).images[0]\n","image.resize((512, 512)).show()\n"],"id":"ee2f8606d8821224","outputs":[],"execution_count":null},{"metadata":{"id":"f346719b561e2a04"},"cell_type":"markdown","source":["As we see, it's starting to do it correctly, but there is definetely some room for improvement.\n"],"id":"f346719b561e2a04"},{"metadata":{"id":"fd10048f78b39b4b"},"cell_type":"markdown","source":["# Submission\n","To determine how well the model performs, we'll evaluate it using another notebook. For this reason, you need to upload the copy of trained pipeline to Hugging Face.\n","\n","1. Register the team at [Hugging Face](https://huggingface.co) or login if you have account alrady.\n","2. Obtain an access token with write rights from [Hugging Face Tokens](https://huggingface.co/settings/tokens).\n","3. In the code below, replace account name with the one you registered and model name with any name you find approprate.\n","4. Enter the access token.\n","\n","Use the [evaluation notebook](https://colab.research.google.com/drive/12eRsJK5AUDoKZOFQo60pzMLdmSJZhl3E) to check the results.\n","\n"],"id":"fd10048f78b39b4b"},{"metadata":{"id":"bc07e41d2c8fd603"},"cell_type":"code","source":["new_pipeline = DiffusionPipeline.from_pretrained(\n","    base_model_name,\n","    vae=vae,\n","    unet=unet,\n",")\n","new_pipeline.push_to_hub(\"your_account/new_model_name\", token='...')\n"],"id":"bc07e41d2c8fd603","outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Testing\n","\n","For this problem, testing will be done entire on our end. Here, you just need to show us how to load your trained model."],"metadata":{"id":"wbSn9PIik4Ni"},"id":"wbSn9PIik4Ni"},{"cell_type":"code","source":["# set variables\n","path_to_model = \"path/to/your/best/model/on/hf\"\n","model_access_token = \"access token\" # a fine-grained token with read rights for your model repository\n","\n","new_pipeline = DiffusionPipeline.from_pretrained(\n","    path_to_model,\n","    token=model_access_token\n",")"],"metadata":{"id":"hJBj-9gXlRdo"},"execution_count":null,"outputs":[],"id":"hJBj-9gXlRdo"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.6"},"colab":{"provenance":[{"file_id":"13TM9Bz7-crji0uRQcSCN1Si8InAmM4lW","timestamp":1719726351615}]}},"nbformat":4,"nbformat_minor":5}