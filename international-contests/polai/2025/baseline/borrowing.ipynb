{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f735706a",
   "metadata": {},
   "source": [
    "# Credit Granting - Model Decision Explanation\n",
    "\n",
    "![](https://live.staticflickr.com/65535/54368714616_e90a2c644c_z.jpg)\n",
    "\n",
    "*Image generated using ChatGPT.*\n",
    "\n",
    "## Introduction\n",
    "Imagine you are a data analyst at a company dealing with credit risk assessment.  \n",
    "Your team has developed a model that, based on key financial indicators,  \n",
    "makes decisions about granting loans to clients. The model works efficiently, but a  \n",
    "problem has emerged – clients who were denied credit are demanding specific explanations.\n",
    "\n",
    "The company management realizes that simply saying \"the computer decided so\"  \n",
    "is not sufficient. They need more – specific suggestions for clients  \n",
    "on what they could change in their financial situation to receive a positive loan decision.\n",
    "\n",
    "As a machine learning specialist, you are asked to develop a system  \n",
    "that helps understand the model's decisions and shows clients a path to getting a loan.\n",
    "\n",
    "\n",
    "## Task\n",
    "Fortunately, to better understand the problem and develop a solution, you will work on a simplified,  \n",
    "two-dimensional dataset. This will enable visualization of results and better understanding of  \n",
    "your explanation system's operation.\n",
    "\n",
    "Your task is to propose a *method for generating explanations* for rejected credit applications, which will suggest realistic changes in the values of financial indicators and ultimately lead to a positive decision by the classifier – a neural network.\n",
    "\n",
    "While working on your solution, you will be able to view results on charts that will show:\n",
    "- The initial position of the observation to be explained;\n",
    "- The classifier's decision boundary;\n",
    "- Suggested changes in the form of vectors and final explanation proposals;\n",
    "- The estimated density distribution of the training data, which will help assess the realism of the proposed changes.\n",
    "\n",
    "### Data\n",
    "The data available to you in this task includes:\n",
    "- A training dataset;\n",
    "- A dataset for explanations;\n",
    "- A discriminative model trained on the training data; this is the model you will explain;\n",
    "- A generative model used to estimate the density distribution of the training data.\n",
    "\n",
    "Specifically, in your method for generating explanations, you may use only the discriminative model, the generative model, and the explanation dataset. The training data is provided only to better illustrate the task goal. \n",
    "\n",
    "Your solution will ultimately be tested on the Competition Platform using a hidden test set, which includes new training data, explanation data, and both discriminative and generative models. The characteristics of the test data will not significantly differ from the dataset provided for building your solution. Additionally, validation data will be available on the Competition Platform, which you can use to ensure your entire solution executes correctly.\n",
    "\n",
    "### Evaluation Criteria\n",
    "As you might expect, the evaluation will focus on three key aspects of your solution:\n",
    "1. **Effectiveness in Changing the Classifier's Decision** – do your proposals actually lead to granting credit;\n",
    "2. **Realism of the Explanations** – are they in a region similar to the training data, i.e., are they achievable for clients;\n",
    "3. **Distance of the Explanations** – are the proposed modifications as small as possible, so as not to burden the client with excessive changes in their financial situation.\n",
    "\n",
    "Because we care about client satisfaction, each of these aspects must exceed a certain threshold for you to earn points. Additionally, each will affect the final score of your solution, according to the formulas presented below, and your final score will be in the range $[0, 100]$.\n",
    "\n",
    "The solution evaluation is based on three main metrics:\n",
    "\n",
    "**Effectiveness in Changing the Classifier’s Decision ($V$)** – A measure indicating the percentage of generated explanations that successfully change the classifier's decision:\n",
    "\n",
    "$$V = \\begin{cases}\n",
    "0, & \\text{if } validity < 0.50 \\\\\n",
    "\\frac{validity - 0.50}{1.00 - 0.50}, & \\text{if } 0.50 \\leq validity \\leq 1.00 \\\\\n",
    "1, & \\text{if } validity > 1.00\n",
    "\\end{cases}$$\n",
    "\n",
    "where *validity* is defined as:\n",
    "$$\\text{validity} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}[f(\\mathbf{{x'}_i}) > 0.5],$$\n",
    "where $\\mathbf{{x'}_i}$ is the proposed explanation for observation $i$, $f$ is the discriminative model, and $N$ is the number of observations.\n",
    "\n",
    "**Realism of the Explanations ($P$)** – A measure indicating the percentage of generated explanations that are considered realistic:\n",
    "\n",
    "$$P = \\begin{cases}\n",
    "0, & \\text{if } plausibility < 0.50 \\\\\n",
    "\\frac{plausibility - 0.50}{1.00 - 0.50}, & \\text{if } 0.50 \\leq plausibility \\leq 1.00 \\\\\n",
    "1, & \\text{if } plausibility > 1.00\n",
    "\\end{cases}$$\n",
    "\n",
    "where *plausibility* is defined as:\n",
    "$$\\text{plausibility} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbb{1}[\\log{P(\\mathbf{{x'}_i}|y')} \\geq \\text{log\\_prob\\_threshold}],$$\n",
    "where $\\log{P(\\mathbf{{x'}_i}|y')}$ is the log-probability of the proposed $i$-th explanation $\\mathbf{{x'}_i}$ given the target class $y'$ – this is the result of the `forward` function of the generative model `gen_model`. The $\\text{log\\_prob\\_threshold}$ is a log-probability threshold that the proposed explanation must exceed and has been previously determined based on the training data.\n",
    "\n",
    "**Distance of the Explanations ($D$)** – A measure indicating how much the proposed changes differ from the original client data:\n",
    "\n",
    "$$D = \\begin{cases} \n",
    "1, & \\text{if } \\text{L2 distance} < 0.22 \\\\\n",
    "\\frac{0.30 - \\text{L2 distance}}{0.30 - 0.22}, & \\text{if } 0.22 \\leq \\text{L2 distance} \\leq 0.30 \\\\\n",
    "0, & \\text{if } \\text{L2 distance} > 0.30.\n",
    "\\end{cases}$$\n",
    "\n",
    "**Final Evaluation Formula**\n",
    "The final score is a combination of the above metrics according to the formula:\n",
    "\n",
    "$$S = 100 \\cdot V \\cdot \\left(\\frac{D}{2} + \\frac{P}{2}\\right)$$\n",
    "\n",
    "This formula expresses that the Effectiveness in Changing the Classifier’s Decision ($V$) is multiplied by the Explanation Distance ($D$) and Explanation Realism ($P$). This means that to get a good score, the solution must effectively change the classifier’s decision while proposing changes that are both realistic and efficient (minimal). The final score $S$ lies within the range $[0, 100]$, where:\n",
    "- Values close to $0$ indicate a weak solution;\n",
    "- Values close to $100$ indicate an excellent solution that effectively changes the classifier’s decisions while maintaining realism and minimal changes.\n",
    "\n",
    "## Constraints\n",
    "- Your solution will be tested on the Competition Platform without internet access.\n",
    "- Evaluation of your final solution on the Competition Platform must not exceed 2 minutes.\n",
    "- Your solution may not use the training set, i.e., `X_train`, `y_train`.\n",
    "- Available libraries: Matplotlib, Numpy, Pandas, PyTorch, Scikit-Learn\n",
    "\n",
    "## Notes and Tips\n",
    "- It is worth changing the loss function to achieve better results.\n",
    "- It is worth using the provided generative model to estimate the density of the training data distribution.\n",
    "\n",
    "## Submission Files\n",
    "This notebook supplemented with your solution (see function `your_generate_explanations`).\n",
    "\n",
    "## Evaluation\n",
    "Remember that during evaluation, the flag `FINAL_EVALUATION_MODE` will be set to `True`.\n",
    "\n",
    "For this task, you can earn between 0 and 100 points. The number of points you will receive will be calculated on the (hidden) test set on the Competition Platform based on the formula mentioned above, rounded to the nearest integer. If your solution does not meet the above criteria or does not execute properly, you will receive 0 points for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b7e7cf",
   "metadata": {},
   "source": [
    "# Starter Code  \n",
    "In this section, we initialize the environment by importing the necessary libraries and functions. The prepared code will help you efficiently operate on the data and build the correct solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a07da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "FINAL_EVALUATION_MODE = False  # During evaluation, we will set this flag to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf278b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "import os\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.inspection import DecisionBoundaryDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb49bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "## Multilayer Perceptron ##\n",
    "\n",
    "class MultilayerPerceptron(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size: int,\n",
    "        hidden_layer_sizes: List[int],\n",
    "        target_size: int,\n",
    "        dropout: float = 0.2,\n",
    "    ):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        self.target_size = target_size\n",
    "        self.input_size = input_size\n",
    "        layer_sizes = [input_size] + hidden_layer_sizes + [target_size]\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(torch.nn.Linear(layer_sizes[i], layer_sizes[i + 1]))\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.target_size = target_size\n",
    "        if target_size == 1:\n",
    "            self.final_activation = torch.nn.Sigmoid()\n",
    "            self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "            self.prep_for_loss = lambda x: x.view(-1, 1).float()\n",
    "        else:\n",
    "            self.final_activation = torch.nn.Softmax(dim=1)\n",
    "            self.criterion = torch.nn.CrossEntropyLoss()\n",
    "            self.prep_for_loss = lambda x: x.view(-1).long()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            if i == len(self.layers) - 1:\n",
    "                x = self.layers[i](x)\n",
    "            else:\n",
    "                x = self.relu(self.layers[i](x))\n",
    "        return x\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        test_loader: torch.utils.data.DataLoader = None,\n",
    "        epochs: int = 200,\n",
    "        lr: float = 0.001,\n",
    "        patience: int = 20,\n",
    "        eps: float = 1e-3,\n",
    "        checkpoint_path: str = \"best_model.pth\",\n",
    "    ):\n",
    "        min_test_loss = float(\"inf\")\n",
    "        optimizer = torch.optim.RAdam(self.parameters(), lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = 0\n",
    "            test_loss = 0\n",
    "            for i, (examples, labels) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.forward(examples)\n",
    "                loss = self.criterion(outputs, self.prep_for_loss(labels))\n",
    "                train_loss += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss /= len(train_loader)\n",
    "            if test_loader:\n",
    "                with torch.no_grad():\n",
    "                    for i, (examples, labels) in enumerate(test_loader):\n",
    "                        outputs = self.forward(examples)\n",
    "                        loss = self.criterion(outputs, self.prep_for_loss(labels))\n",
    "                        test_loss += loss.item()\n",
    "                        # Early stopping\n",
    "                    test_loss /= len(test_loader)\n",
    "                if test_loss < (min_test_loss - eps):\n",
    "                    min_test_loss = test_loss\n",
    "                    patience_counter = 0\n",
    "                    self.save(checkpoint_path)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "                if patience_counter > patience:\n",
    "                    break\n",
    "                self.load(checkpoint_path)\n",
    "            print(\n",
    "                f\"Epoch {epoch}, Train: {train_loss:.4f}, test: {test_loss:.4f}, patience: {patience_counter}\"\n",
    "            )\n",
    "\n",
    "    def predict(self, X_test: Union[np.ndarray, torch.Tensor]):\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            probs = self.predict_proba(X_test) > 0.5\n",
    "            return probs.squeeze().float()\n",
    "\n",
    "    def predict_proba(self, X_test: Union[np.ndarray, torch.Tensor]):\n",
    "        if isinstance(X_test, np.ndarray):\n",
    "            X_test = torch.from_numpy(X_test).float()\n",
    "        with torch.no_grad():\n",
    "            logits = self.forward(X_test)\n",
    "            probs = self.final_activation(logits)\n",
    "            return probs.float()\n",
    "\n",
    "    def save(self, path: str):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path: str):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "\n",
    "\n",
    "## Kernel Density Estimator ##\n",
    "\n",
    "class GaussianKernel(nn.Module):\n",
    "    \"\"\"Implementation of the Gaussian kernel.\"\"\"\n",
    "    \n",
    "    def __init__(self, bandwidth=1.0):\n",
    "        \"\"\"Initializes a new Kernel.\n",
    "\n",
    "        Args:\n",
    "            bandwidth: The kernel's (band)width.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def _diffs(self, test_Xs, train_Xs):\n",
    "        \"\"\"Computes difference between each x in test_Xs with all train_Xs.\"\"\"\n",
    "        test_Xs = test_Xs.view(test_Xs.shape[0], 1, *test_Xs.shape[1:])\n",
    "        train_Xs = train_Xs.view(1, train_Xs.shape[0], *train_Xs.shape[1:])\n",
    "        return test_Xs - train_Xs\n",
    "\n",
    "    def forward(self, test_Xs, train_Xs):\n",
    "        \"\"\"Computes log p(x) for each x in test_Xs given train_Xs.\"\"\"\n",
    "        n, d = train_Xs.shape\n",
    "        n, h = torch.tensor(n, dtype=torch.float32), torch.tensor(self.bandwidth)\n",
    "        pi = torch.tensor(np.pi)\n",
    "\n",
    "        Z = 0.5 * d * torch.log(2 * pi) + d * torch.log(h) + torch.log(n)\n",
    "        diffs = self._diffs(test_Xs, train_Xs) / h\n",
    "        log_exp = -0.5 * torch.norm(diffs, p=2, dim=-1) ** 2\n",
    "\n",
    "        return torch.logsumexp(log_exp - Z, dim=-1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, train_Xs):\n",
    "        \"\"\"Generates samples from the kernel distribution.\"\"\"\n",
    "        device = train_Xs.device\n",
    "        noise = torch.randn(train_Xs.shape, device=device) * self.bandwidth\n",
    "        return train_Xs + noise\n",
    "\n",
    "\n",
    "class KernelDensityEstimator(nn.Module):\n",
    "    \"\"\"The KernelDensityEstimator model.\"\"\"\n",
    "\n",
    "    def __init__(self, train_Xs, kernel=None):\n",
    "        \"\"\"Initializes a new KernelDensityEstimator.\n",
    "\n",
    "        Args:\n",
    "            train_Xs: The \"training\" data to use when estimating probabilities.\n",
    "            kernel: The kernel to place on each of the train_Xs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel = kernel or GaussianKernel()\n",
    "        self.train_Xs = nn.Parameter(train_Xs, requires_grad=False)\n",
    "        assert len(self.train_Xs.shape) == 2, \"Input cannot have more than two axes.\"\n",
    "\n",
    "    def __call__(self, x, *args, **kwargs):\n",
    "        \"\"\"Saves input tensor attributes so they can be accessed during sampling.\"\"\"\n",
    "        if getattr(self, \"_c\", None) is None and x.dim() == 4:\n",
    "            _, c, h, w = x.shape\n",
    "            self._create_shape_buffers(c, h, w)\n",
    "        return super().__call__(x, *args, **kwargs)\n",
    "\n",
    "    def load_state_dict(self, state_dict, strict=True):\n",
    "        \"\"\"Registers dynamic buffers before loading the model state.\"\"\"\n",
    "        if \"_c\" in state_dict and not getattr(self, \"_c\", None):\n",
    "            c, h, w = state_dict[\"_c\"], state_dict[\"_h\"], state_dict[\"_w\"]\n",
    "            self._create_shape_buffers(c, h, w)\n",
    "        super().load_state_dict(state_dict, strict)\n",
    "\n",
    "    def _create_shape_buffers(self, channels, height, width):\n",
    "        channels = channels if torch.is_tensor(channels) else torch.tensor(channels)\n",
    "        height = height if torch.is_tensor(height) else torch.tensor(height)\n",
    "        width = width if torch.is_tensor(width) else torch.tensor(width)\n",
    "        self.register_buffer(\"_c\", channels)\n",
    "        self.register_buffer(\"_h\", height)\n",
    "        self.register_buffer(\"_w\", width)\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.train_Xs.device\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.kernel(x, self.train_Xs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, n_samples):\n",
    "        idxs = np.random.choice(range(len(self.train_Xs)), size=n_samples)\n",
    "        return self.kernel.sample(self.train_Xs[idxs])\n",
    "\n",
    "\n",
    "class KDE(torch.nn.Module):\n",
    "    def __init__(self, bandwidth=0.1, **kwargs):  # Ignores kwargs!\n",
    "        super(KDE, self).__init__()\n",
    "        self.bandwidth = bandwidth\n",
    "        self.models = nn.ModuleDict()\n",
    "\n",
    "    def _context_to_key(self, context):\n",
    "        return str(int(context))\n",
    "\n",
    "    def _get_model_for_context(self, context):\n",
    "        key = self._context_to_key(context)\n",
    "        if key not in self.models:\n",
    "            raise ValueError(f\"Context {key} not found in the model.\")\n",
    "        return self.models[key]\n",
    "\n",
    "    def load_state_dict(\n",
    "            self,\n",
    "            state_dict,\n",
    "            strict: bool = True,\n",
    "            assign: bool = False,\n",
    "    ):\n",
    "        for key in state_dict.keys():\n",
    "            if key.startswith(\"models.\"):\n",
    "                self.models[key.split(\".\")[1]] = KernelDensityEstimator(\n",
    "                    state_dict[key], kernel=GaussianKernel(bandwidth=self.bandwidth)\n",
    "                )\n",
    "        return super().load_state_dict(state_dict, strict, assign)\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            train_loader: torch.utils.data.DataLoader,\n",
    "            test_loader: torch.utils.data.DataLoader,\n",
    "            checkpoint_path: str = \"best_model.pth\",\n",
    "            **kwargs,\n",
    "    ):\n",
    "        train_Xs, train_ys = train_loader.dataset.tensors\n",
    "        train_ys = train_ys.view(-1)\n",
    "        for y in train_ys.unique():\n",
    "            idxs = train_ys == y\n",
    "            self.models.update(\n",
    "                {\n",
    "                    self._context_to_key(y.item()): KernelDensityEstimator(\n",
    "                        train_Xs[idxs], kernel=GaussianKernel(bandwidth=self.bandwidth)\n",
    "                    )\n",
    "                }\n",
    "            )\n",
    "        self.save(checkpoint_path)\n",
    "\n",
    "        train_log_probs = self.predict_log_prob(train_loader)\n",
    "        test_log_probs = self.predict_log_prob(test_loader)\n",
    "        print(f\"Train log-likelihood: {train_log_probs.float().mean()}\")\n",
    "        print(f\"Test log-likelihood: {test_log_probs.float().mean()}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, context: torch.Tensor):\n",
    "        preds = torch.zeros_like(context)\n",
    "        for i in range(x.shape[0]):\n",
    "            model = self._get_model_for_context(context[i].item())\n",
    "            preds[i] = model(x[i].unsqueeze(0))\n",
    "        return preds.view(-1)\n",
    "\n",
    "    def predict_log_prob(self, dataloader: torch.utils.data.DataLoader):\n",
    "        inputs, context = dataloader.dataset.tensors\n",
    "        preds = self(inputs, context)\n",
    "        preds = torch.zeros_like(context, dtype=torch.float32)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            model = self._get_model_for_context(context[i].item())\n",
    "            preds[i] = model(inputs[i].unsqueeze(0))\n",
    "        return preds\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1112a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "# Cell containing helper functions for plotting graphs.\n",
    "\n",
    "## HELPER FUNCTIONS ##\n",
    "\n",
    "def _plot_generative_model_distribution(ax, model, log_prob_threshold=None):\n",
    "    xline = torch.linspace(-0, 1, 200)\n",
    "    yline = torch.linspace(-0, 1, 200)\n",
    "    xgrid, ygrid = torch.meshgrid(xline, yline)\n",
    "    xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        zgrid = model(xyinput, torch.ones(40000, 1)).exp().reshape(200, 200)\n",
    "        zgrid = zgrid.numpy()\n",
    "        _ = ax.contour(\n",
    "            xgrid.numpy(),\n",
    "            ygrid.numpy(),\n",
    "            zgrid,\n",
    "            levels=10,\n",
    "            cmap=\"Greys\",\n",
    "            linewidths=0.4,\n",
    "            antialiased=True,\n",
    "        )\n",
    "        ax.plot([], [], color='grey', alpha=0.3, label=\"Training data density contours\")\n",
    "\n",
    "    if log_prob_threshold is not None:\n",
    "        prob_threshold_exp = np.exp(log_prob_threshold)\n",
    "        _ = ax.contourf(\n",
    "            xgrid.numpy(),\n",
    "            ygrid.numpy(),\n",
    "            zgrid,\n",
    "            levels=[prob_threshold_exp, prob_threshold_exp * 10.00],\n",
    "            alpha=0.1,\n",
    "            colors=\"#DC143C\",\n",
    "        )  # 10.00 is an arbitrary huge value to colour the whole distribution.\n",
    "        ax.plot([], [], color='#DC143C', alpha=0.3, label=\"Realism area of explanations\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_classifier_decision_region(ax, model):\n",
    "    xline = torch.linspace(0, 1, 1000)\n",
    "    yline = torch.linspace(0, 1, 1000)\n",
    "    xgrid, ygrid = torch.meshgrid(xline, yline, indexing=\"ij\")\n",
    "    xyinput = torch.cat([xgrid.reshape(-1, 1), ygrid.reshape(-1, 1)], dim=1)\n",
    "\n",
    "    y_hat = model.predict(xyinput)\n",
    "    y_hat = y_hat.reshape(1000, 1000)\n",
    "\n",
    "    display = DecisionBoundaryDisplay(xx0=xgrid, xx1=ygrid, response=y_hat)\n",
    "    display.plot(plot_method=\"contour\", ax=ax, alpha=0.3)\n",
    "    ax.plot([], [], color='green', alpha=0.3, label=\"Model decision boundary\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_propositions(ax, propositions):\n",
    "    ax.scatter(\n",
    "        propositions[:, 0], \n",
    "        propositions[:, 1], \n",
    "        c=\"orange\", \n",
    "        s=50, \n",
    "        alpha=0.8,\n",
    "        label=\"Explanations\"\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_observations(ax, observations, targets):\n",
    "    indices = targets == 0\n",
    "\n",
    "    observations_0 = observations[indices]\n",
    "    observations_1 = observations[~indices]\n",
    "    \n",
    "    ax.scatter(\n",
    "        observations_0[:, 0],\n",
    "        observations_0[:, 1],\n",
    "        c=\"blue\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        label=\"Observations from class 0\"\n",
    "    )\n",
    "\n",
    "    ax.scatter(\n",
    "        observations_1[:, 0],\n",
    "        observations_1[:, 1],\n",
    "        c=\"red\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        label=\"Observations from class 1\"\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_observations_to_explain(ax, observations):\n",
    "    ax.scatter(\n",
    "        observations[:, 0],\n",
    "        observations[:, 1],\n",
    "        c=\"blue\",\n",
    "        s=50,\n",
    "        alpha=0.8,\n",
    "        label=\"Observations to explain\"\n",
    "    )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def _plot_arrows(ax, observations, propositions):\n",
    "    for i in range(len(observations)):\n",
    "        ax.arrow(\n",
    "            observations[i, 0],\n",
    "            observations[i, 1],\n",
    "            propositions[i, 0] - observations[i, 0],\n",
    "            propositions[i, 1] - observations[i, 1],\n",
    "            width=0.001,\n",
    "            lw=0.001,\n",
    "            length_includes_head=True,\n",
    "            alpha=0.5,\n",
    "            color=\"k\",\n",
    "        )\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_initial_setup(X_orig, y_orig, disc_model=None, gen_model=None, log_prob_threshold=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(20, 12)\n",
    "\n",
    "    ax = _plot_observations(ax, X_orig, y_orig)\n",
    "\n",
    "    if disc_model:\n",
    "        ax = _plot_classifier_decision_region(ax, disc_model)\n",
    "    if gen_model:\n",
    "        ax = _plot_generative_model_distribution(\n",
    "            ax, gen_model, log_prob_threshold=log_prob_threshold\n",
    "        )\n",
    "\n",
    "    ax.legend(loc=\"lower left\")\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_explanation_setup(X_orig, X_new=None, disc_model=None, gen_model=None, log_prob_threshold=None):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    fig.set_size_inches(20, 12)\n",
    "\n",
    "    ax = _plot_observations_to_explain(ax, X_orig)\n",
    "    \n",
    "    if disc_model:\n",
    "        ax = _plot_classifier_decision_region(ax, disc_model)\n",
    "    \n",
    "    if gen_model:\n",
    "        ax = _plot_generative_model_distribution(\n",
    "            ax, gen_model, log_prob_threshold=log_prob_threshold\n",
    "        )\n",
    "    \n",
    "    if X_new is not None:\n",
    "        assert (\n",
    "                X_orig.shape == X_new.shape\n",
    "        ), f\"Sizes of test set and counterfactuals are not equal. Actual sizes: X_orig: {X_orig.shape}, X_cf: {X_new.shape}\"\n",
    "\n",
    "        ax = _plot_propositions(ax, X_new)\n",
    "        ax = _plot_arrows(ax, X_orig, X_new)\n",
    "    \n",
    "    ax.legend(loc=\"lower left\")\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a73dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "DATA_DIRECTORY = 'data'\n",
    "\n",
    "ARRAY_X_TRAIN_PATH = f'{DATA_DIRECTORY}/x_train.npy'\n",
    "ARRAY_X_EXPLAIN_PATH = f'{DATA_DIRECTORY}/x_explain.npy'\n",
    "ARRAY_Y_TRAIN_PATH = f'{DATA_DIRECTORY}/y_train.npy'\n",
    "\n",
    "DISC_MODEL_PATH = f'{DATA_DIRECTORY}/disc_model.pth'\n",
    "GEN_MODEL_PATH = f'{DATA_DIRECTORY}/gen_model.pth'\n",
    "LOG_PROB_THRESHOLD_PATH = f'{DATA_DIRECTORY}/log_prob_threshold.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92583843",
   "metadata": {},
   "source": [
    "## Loading Data  \n",
    "In this part of the task, we will load the training data that was used to train the discriminative model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ab78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "X_train = np.load(ARRAY_X_TRAIN_PATH)\n",
    "y_train = np.load(ARRAY_Y_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eb11a8",
   "metadata": {},
   "source": [
    "Let's display the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f2b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_initial_setup(X_train, y_train)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3426d",
   "metadata": {},
   "source": [
    "## Loading the Discriminative Model\n",
    "\n",
    "In this task, we will explain a simple neural network model that has been previously trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05417549",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "disc_model = MultilayerPerceptron(\n",
    "    input_size=2, \n",
    "    hidden_layer_sizes=[256, 256], \n",
    "    target_size=1, \n",
    "    dropout=0.1\n",
    ")\n",
    "disc_model.load(DISC_MODEL_PATH)\n",
    "disc_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fbc677",
   "metadata": {},
   "source": [
    "Let's display the dataset and the model's decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78445157",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_initial_setup(X_train, y_train, disc_model=disc_model)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f89e1b",
   "metadata": {},
   "source": [
    "## Realism of Explanations\n",
    "\n",
    "In this task, we will focus on an important aspect of generating explanations — we want the generated points to be realistic, and in our case, we will define this as originating from a region of high density in the training data distribution.\n",
    "\n",
    "Let's start by getting acquainted with the problem of density estimation. Density estimation is the task of finding a function $p(x)$ that approximates the true probability distribution of the data $p^*(x)$. Formally, given a set of samples $\\{x_1, ..., x_n\\}$ drawn from an unknown distribution $p^*(x)$, we want to find a model $p(x)$ that best approximates this distribution.\n",
    "\n",
    "In this task, we will use a kernel density estimator (KDE), which is one of the most popular models for density estimation. As a criterion for the acceptability threshold of realism, we will use the median of the density function values for the training points, which has been precomputed for you. This means that the KDE density function for the proposed new variables for the client should have a value above this acceptability threshold. This concept is visualized in the next plot as a red area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f6bbc",
   "metadata": {},
   "source": [
    "## Loading the Generative Model along with the Acceptability Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa0493",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "gen_model = KDE(bandwidth=0.05)\n",
    "gen_model.load(GEN_MODEL_PATH)\n",
    "gen_model.eval()\n",
    "\n",
    "with open(LOG_PROB_THRESHOLD_PATH, 'r') as f:\n",
    "    log_prob_threshold = float(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821805b6",
   "metadata": {},
   "source": [
    "Display the setup of the model, data, and data distribution density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ca384",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_initial_setup(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        disc_model=disc_model,\n",
    "        gen_model=gen_model,\n",
    "        log_prob_threshold=log_prob_threshold\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52328ef",
   "metadata": {},
   "source": [
    "## Loading Data for Explanation\n",
    "In this part of the task, we will load the dataset for explanation. Your task will be to generate explanations for the points from this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e39a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "X_explain = np.load(ARRAY_X_EXPLAIN_PATH)\n",
    "y_explain = np.ones((X_explain.shape[0], 1))  # Vector 1 - the class to which we want to change the model's decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a5dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_explanation_setup(X_explain, disc_model=disc_model, gen_model=gen_model, log_prob_threshold=log_prob_threshold)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7adab8",
   "metadata": {},
   "source": [
    "## Example Solution\n",
    "Below we present a simplified solution that serves as an example demonstrating the basic functionality of the notebook. It can be used as a starting point for developing your own solution.\n",
    "\n",
    "One way to solve the above problem is to optimize the target point $ x^* $ by minimizing the following objective function:\n",
    "\n",
    "$$\n",
    "L(x^*) = \\text{BCE}(f(x^*), y^*) + \\lambda \\cdot \\|x^* - x\\|_2^2\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\text{BCE}$ is the binary cross-entropy loss function,\n",
    "- $f(x^*)$ is the model's prediction for the point $ x^* $,\n",
    "- $y^*$ is the desired target class,\n",
    "- $\\|x^* - x\\|_2^2$ is the squared Euclidean distance between the point $ x^* $ and the original point $ x $,\n",
    "- $\\lambda$ is a parameter regulating the trade-off between components of the loss function (in the implementation $\\lambda = 0.1$).\n",
    "\n",
    "This is a basic approach that does not take into account the training data distribution. Below you will find an example implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ab007",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "def example_generate_explanations(\n",
    "        X_explain: np.ndarray, \n",
    "        y_explain: np.ndarray, \n",
    "        disc_model: MultilayerPerceptron,\n",
    "        gen_model: KDE,\n",
    "        log_prob_threshold: float,\n",
    "        verbose: bool = False\n",
    "    ) -> np.ndarray:\n",
    "    \n",
    "    num_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "\n",
    "    x_orig = torch.tensor(X_explain, dtype=torch.float32)\n",
    "    target = torch.tensor(y_explain, dtype=torch.float32)\n",
    "\n",
    "    # Definicja nowych punktów X jako wyjaśnienia.\n",
    "    x_new = torch.tensor(X_explain, requires_grad=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam([x_new], lr=lr)\n",
    "    bce_loss = torch.nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Prediction Loss (Binary Cross Entropy)\n",
    "        pred = disc_model(x_new)\n",
    "        pred_loss = bce_loss(pred, target)\n",
    "\n",
    "        # Distance Loss (Squared L2)\n",
    "        dist_loss = torch.sum((x_orig - x_new)**2, axis=1, keepdim=True)\n",
    "\n",
    "        # Total Loss\n",
    "        total_loss = pred_loss + 0.1 * dist_loss\n",
    "        total_loss = total_loss.mean()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Total loss: {total_loss:.4f}\")\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    return x_new.detach().numpy()\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    X_new = example_generate_explanations(X_explain, y_explain, disc_model=disc_model, gen_model=gen_model, log_prob_threshold=log_prob_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616625ed",
   "metadata": {},
   "source": [
    "## Explanation Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b0776",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "# Visualization of Generated Explanations\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    fig, ax = plot_explanation_setup(\n",
    "        X_explain,\n",
    "        X_new=X_new,\n",
    "        disc_model=disc_model,\n",
    "        gen_model=gen_model,\n",
    "        log_prob_threshold=log_prob_threshold\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b15cc8",
   "metadata": {},
   "source": [
    "# Your Solution\n",
    "In this section, you should place your solution. Make changes only here!\n",
    "\n",
    "Your task is to implement the function ```your_generate_explanations```.\n",
    "Remember that the function definition should not be changed, and the resulting array of outputs should be the same size as the input array of points to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffab406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_generate_explanations(\n",
    "        X_explain: np.ndarray, \n",
    "        y_explain: np.ndarray, \n",
    "        disc_model: MultilayerPerceptron,\n",
    "        gen_model: KDE,\n",
    "        log_prob_threshold: float\n",
    "    ) -> np.ndarray:\n",
    "\n",
    "    results = X_explain\n",
    "\n",
    "    assert results.shape == X_explain.shape\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775f907",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "Running the cell below will allow you to check how many points your solution would score on the available data. Before submitting, make sure the entire notebook runs from start to finish without errors and without requiring user intervention after selecting \"Run All\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616166d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "def scale(x, lower=0.50, upper=1.00, max_points=1.0):\n",
    "    \"\"\"Function that linearly scales the score.\"\"\"\n",
    "    scaled = min(max(x, lower), upper)\n",
    "    return (scaled - lower) / (upper - lower) * max_points\n",
    "\n",
    "\n",
    "def calculate_average_distance(X_orig, X_new):\n",
    "    \"\"\"Calculate the average L2 distance between points.\"\"\"\n",
    "    distances = np.sqrt(np.sum((X_orig - X_new)**2, axis=1))\n",
    "    return np.mean(distances)\n",
    "\n",
    "\n",
    "def calculate_validity_criterion(disc_model, X_new):\n",
    "    \"\"\"Calculate the percentage of observations correctly crossing the decision boundary.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        cf_preds = disc_model.predict(X_new)\n",
    "    return np.mean((cf_preds > 0.5).numpy())\n",
    "\n",
    "\n",
    "def calculate_plausibility_criterion(gen_model, X_new, log_prob_threshold):\n",
    "    \"\"\"Calculate the percentage of observations above the plausibility threshold.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        cf_log_probs = gen_model(torch.tensor(X_new, dtype=torch.float32), torch.ones((X_new.shape[0], 1)))\n",
    "        return torch.mean((cf_log_probs >= log_prob_threshold).float()).item()\n",
    "\n",
    "\n",
    "def calculate_final_metric(X_explain, X_new, disc_model, gen_model, verbose=True):\n",
    "    \"\"\"Calculate the final metric.\"\"\"\n",
    "    LOG_PROB_THRESHOLD = log_prob_threshold\n",
    "    DISTANCE_UPPER_BOUND = 0.30\n",
    "    DISTANCE_LOWER_BOUND = 0.22\n",
    "    VALIDITY_UPPER_BOUND = 1.00\n",
    "    VALIDITY_LOWER_BOUND = 0.50\n",
    "    PLAUSIBILITY_UPPER_BOUND = 1.00\n",
    "    PLAUSIBILITY_LOWER_BOUND = 0.50\n",
    "    \n",
    "    avg_distance = calculate_average_distance(X_explain, X_new)\n",
    "    distances = 1 if avg_distance < DISTANCE_LOWER_BOUND else max(0, min(1, (DISTANCE_UPPER_BOUND - avg_distance) / (DISTANCE_UPPER_BOUND - DISTANCE_LOWER_BOUND)))\n",
    "\n",
    "    validity_rate = calculate_validity_criterion(disc_model, X_new)\n",
    "    validity_rate = scale(validity_rate, VALIDITY_LOWER_BOUND, VALIDITY_UPPER_BOUND)\n",
    "    \n",
    "    plausibility_rate = calculate_plausibility_criterion(gen_model, X_new, LOG_PROB_THRESHOLD)\n",
    "    plausibility_rate = scale(plausibility_rate, PLAUSIBILITY_LOWER_BOUND, PLAUSIBILITY_UPPER_BOUND)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Average distance: {avg_distance:.4f}\\n\")\n",
    "        print(f\"Score: Explanation distance: {distances:.4f}\")\n",
    "        print(f\"Score: Classifier decision change effectiveness: {validity_rate:.4f}\")\n",
    "        print(f\"Score: Explanation plausibility effectiveness: {plausibility_rate:.4f}\")\n",
    "        print(\"-\"*30)\n",
    "    \n",
    "    # Calculating the final metric\n",
    "    score = 100 * validity_rate * (plausibility_rate + distances) / 2\n",
    "    final_metric = int(round(score))\n",
    "    print(f\"Estimated points for the task: {final_metric}\")\n",
    "    return final_metric\n",
    "\n",
    "\n",
    "if not FINAL_EVALUATION_MODE:\n",
    "    X_new = your_generate_explanations(X_explain, y_explain, disc_model=disc_model, gen_model=gen_model, log_prob_threshold=log_prob_threshold)\n",
    "    final_score = calculate_final_metric(X_explain, X_new, disc_model, gen_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2be889",
   "metadata": {},
   "source": [
    "During evaluation, the model will be saved as `your_model.pkl` and assessed on the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25173d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DO NOT CHANGE THIS CELL ##########################\n",
    "\n",
    "if FINAL_EVALUATION_MODE:\n",
    "    import cloudpickle\n",
    "\n",
    "    OUTPUT_PATH = \"file_output\"\n",
    "    FUNCTION_FILENAME = \"your_model.pkl\"\n",
    "    FUNCTION_OUTPUT_PATH = os.path.join(OUTPUT_PATH, FUNCTION_FILENAME)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_PATH):\n",
    "        os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "    with open(FUNCTION_OUTPUT_PATH, \"wb\") as f:\n",
    "        cloudpickle.dump(your_generate_explanations, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
