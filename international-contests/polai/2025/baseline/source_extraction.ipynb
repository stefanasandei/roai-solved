{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ad74adf3",
            "metadata": {
                "id": "ad74adf3"
            },
            "source": [
                "# Source Extraction\n",
                "\n",
                "<img src=\"https://live.staticflickr.com/65535/54443002259_4a8e1249dd_b.jpg\" alt=\"Embedded Photo\" width=\"500\">\n",
                "\n",
                "*Image generated using ChatGPT.*\n",
                "\n",
                "## Introduction\n",
                "\n",
                "Language models are often prone to telling untruths or half-truths, as well as fabricating facts without providing sources. Nowadays, systems are increasingly used that, instead of answering questions directly, first search a database, e.g., a collection of documents, and then generate an answer based on the best-matching documents. Such an answer is more likely to be grounded in reality and can be verified by a human—provided the correct sources have been properly found.\n",
                "\n",
                "Of course, there can be a huge number of sources, so search methods must be efficient—processing everything \"at once\" directly with a language model is out of the question! In this task, you will focus on finding the best sources for a given sentence using the method of **embeddings** (vector representations).\n",
                "\n",
                "Imagine that you are an AI engineer at a company developing a tool for scientific fact verification. Your task is to build a module that can quickly and effectively find reliable scientific publications that confirm or refute specific claims. Thanks to your solution, scientists, journalists, and policymakers will be able to verify information based on solid scientific foundations, which is especially important in the age of disinformation.\n",
                "\n",
                "## Task\n",
                "\n",
                "Your task is to develop a system that generates high-quality vector representations (embeddings) for both queries and source documents, enabling precise matching of relevant sources to queries.\n",
                "\n",
                "Given **queries** (a set of questions; queries for which we seek sources) and a **corpus** (a database of documents/sources; a set of considered documents), you must implement functions that assign queries and sources real-valued vectors of dimension $768$. These vectors will be used to find sources for each query using a provided evaluation function, which selects the $k=10$ nearest neighbors (k-Nearest Neighbors) from the document set.\n",
                "\n",
                "You may use the provided model based on the GPT2 architecture, which has been specially fine-tuned to produce high-quality embeddings.\n",
                "\n",
                "While working on your solution, you will be able to test its effectiveness on a validation set, which will allow you to evaluate the quality of the generated embeddings in the context of finding appropriate source documents.\n",
                "\n",
                "### Data\n",
                "\n",
                "The available data in this task includes:\n",
                "\n",
                "- A set of queries, for which appropriate sources must be found\n",
                "- A corpus of documents, containing scientific publications that can be sources for the queries\n",
                "- Information on query-to-document matches in the validation set\n",
                "\n",
                "Your solution will be evaluated on the *SciFact* benchmark. It is used to assess search and fact verification systems in a scientific context. It consists of a set of statements (queries) based on real scientific publications, and the document base (corpus) consists of publications in the natural and medical sciences. For each statement, there is at least one publication that supports or refutes it. We provide code to load the data, so the data is described here for informational purposes only.\n",
                "\n",
                "**The `corpus.jsonl` file** contains unique identifiers, titles, and abstracts of scientific papers\n",
                "\n",
                "Example of a single document:\n",
                "```\n",
                "{\n",
                "    \"text_id\": 13734012,\n",
                "    \"title\": \"Prevalent abnormal prion protein in human appendixes after bovine spongiform encephalopathy epizootic: large scale survey\",\n",
                "    \"text\": \"OBJECTIVES To carry out a further survey (...) CONCLUSIONS This study corroborates previous studies and suggests a high prevalence of infection with abnormal PrP, indicating vCJD carrier status in the population compared with the 177 vCJD cases to date. These findings have important implications for the management of blood and blood products and for the handling of surgical instruments.\"\n",
                "}\n",
                "```\n",
                "\n",
                "**The `queries_val.jsonl` file** contains the content of the statements and the identifier of the matching source text. The test set, on which your final solution will be evaluated, **will not contain** matching text identifiers.\n",
                "\n",
                "Example of a single query:\n",
                "```\n",
                "{\n",
                "    \"query\": \"1 in 5 million in UK have abnormal PrP positivity.\",\n",
                "    \"matching_text_id\": 13734012\n",
                "}\n",
                "```\n",
                "\n",
                "### Evaluation Criteria\n",
                "The methods (functions) you implement, `Embedder.encode_queries` and `Embedder.encode_corpus`, will be used to process the queries $q \\in Q$ and documents $d \\in C$ into vectors. We will interchangeably use $q$ and $d$ to refer to both texts and their embeddings.\n",
                "\n",
                "Let us assume that query $q\\in Q$ corresponds to the gold document $d\\in C$.\n",
                "The evaluation code sorts all documents by distance to $q$, resulting in documents $K_1, K_2, ..., K_n$, such that $K_1$ is the closest. We denote by $I$ the index of the gold document $d$ in this sequence. This means that $I - 1$ is the number of documents whose distance to $q$ is less than the distance from $q$ to $d$.\n",
                "\n",
                "The distance between vectors is measured using cosine similarity, which for vectors $v, w \\in \\mathbb{R}^n$ is defined as $\\frac{v^Tw}{||v|| \\cdot ||w||}$, where $||v||$ is the length of vector $v$.\n",
                "\n",
                "The result for query $q$ is defined as  \n",
                "\n",
                "$$\\text{nDCG@10}(q) = \\begin{cases}\n",
                "\\frac{1}{\\log_2(I + 1)} & \\text{if $I \\leq 10$} \\\\\n",
                "0 & \\text{otherwise.}\n",
                "\\end{cases}$$\n",
                "\n",
                "So, the closer the gold document is ranked to the query compared to other documents, the higher the score—if 10 \"wrong\" documents are closer to the query, the score for that example is 0.\n",
                "\n",
                "Your final solution will be scored based on the **nDCG@10** metric, calculated as the average value of this metric over all queries $(q \\in Q )$.\n",
                "\n",
                "- If the **nDCG@10** score is **less than 0.2**, you will receive **0 points**.  \n",
                "- If the score **exceeds 0.5**, you will receive the **maximum score**, which is **100**.  \n",
                "\n",
                "Scoring for values between these thresholds will be calculated proportionally.\n",
                "\n",
                "## Constraints\n",
                "\n",
                "- Your solution will be tested on the Competition Platform without internet access and in a GPU environment.\n",
                "- The final evaluation of your solution on the Competition Platform must not exceed 10 minutes using a GPU.\n",
                "- The embedding of each query and document must have a dimension of 768\n",
                "- Allowed libraries: `torch`, `pandas`, `numpy`, `nltk`, `transformers`\n",
                "\n",
                "## Submission Files\n",
                "\n",
                "You must submit only this notebook filled in with your solution (see the `Embedder` class).\n",
                "\n",
                "## Tips\n",
                "\n",
                "- GPT2 is a decoder-style language model. Decoder models work such that for a given sequence of tokens (e.g., a prefix of a sentence) $t_1, t_2, \\dots, t_n$ they compute a hidden vector $h_{n+1} \\in \\mathbb{R}^d$, and then transform it with one of their weight matrices into $p_{n+1} \\in \\mathbb{R}^m$—a probability distribution over the vocabulary tokens.\n",
                "- There are many documents compared to the available execution time.\n",
                "\n",
                "## Evaluation\n",
                "\n",
                "During final evaluation, the flag `FINAL_EVALUATION_MODE` will be set to `True`.\n",
                "\n",
                "You can earn between 0 and 100 points for this task. The number of points you receive will be calculated on a (secret) test set on the Competition Platform based on the above formula, rounded to the nearest integer. If your solution does not meet the above criteria or does not execute correctly, you will receive 0 points for the task."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "01485926",
            "metadata": {
                "id": "01485926"
            },
            "source": [
                "# Starter Code\n",
                "\n",
                "In this section, we initialize the environment by importing the necessary libraries and functions. The prepared tokenizer, data loading, and evaluation code will help you operate on the data and solve the task."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "1e81e8ad",
            "metadata": {
                "id": "1e81e8ad"
            },
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "FINAL_EVALUATION_MODE = False  # During the evaluation of your solution, we will set this value to True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "00d2c005",
            "metadata": {},
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "import json\n",
                "import os\n",
                "from math import log2\n",
                "\n",
                "import torch\n",
                "from tqdm import tqdm\n",
                "from transformers import AutoModel, AutoTokenizer\n",
                "\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "\n",
                "class Tokenizer:\n",
                "    def __init__(self, tokenizer_path, length=150):\n",
                "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
                "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
                "        self.tokenizer.padding_side = \"right\"\n",
                "        self.length = length\n",
                "\n",
                "    def __call__(self, batch_text):\n",
                "        batch_tensor = self.tokenizer(\n",
                "            batch_text,\n",
                "            max_length=self.length,\n",
                "            truncation=True,\n",
                "            padding=True,\n",
                "            return_tensors=\"pt\"\n",
                "        )\n",
                "        return batch_tensor.to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "569c20b9",
            "metadata": {},
            "source": [
                "## Loading Data  \n",
                "In this part of the task, we will load the training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7976d6e1",
            "metadata": {},
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "def load_corpus(file):\n",
                "    corpus = {}\n",
                "    with open(file, encoding=\"utf8\") as f_in:\n",
                "        for line in f_in:\n",
                "            line = json.loads(line)\n",
                "            corpus[line.get(\"text_id\")] = {\n",
                "                \"text\": line.get(\"text\"),\n",
                "                \"title\": line.get(\"title\"),\n",
                "            }\n",
                "    return corpus\n",
                "\n",
                "def load_queries(file):\n",
                "    queries = {}\n",
                "    matching_texts = {}\n",
                "    with open(file, encoding=\"utf8\") as f_in:\n",
                "        for query_num, line in enumerate(f_in):\n",
                "            line = json.loads(line)\n",
                "\n",
                "            queries[query_num] = line.get(\"query\")\n",
                "            matching_texts[query_num] = line.get(\"matching_text_id\")\n",
                "    return queries, matching_texts\n",
                "\n",
                "corpus = load_corpus(\"corpus.jsonl\")\n",
                "queries, matching_texts = load_queries(\"queries_val.jsonl\")\n",
                "\n",
                "print(f\"Loaded {len(corpus)} texts and {len(queries)} queries.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "19bda51c",
            "metadata": {},
            "source": [
                "## Evaluation Criterion Code\n",
                "\n",
                "Code similar to the one below will be used to evaluate the solution on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f414e30a",
            "metadata": {},
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "def evaluate_retrieval_ndcg(\n",
                "    golden_matches: dict[int, int],\n",
                "    results: dict[int, dict[int, float]],\n",
                ") -> float:\n",
                "    \"\"\"\n",
                "    Computes the nDCG metric value for the given search results.\n",
                "\n",
                "    This function calculates the score of your solution based on the results for the top_k best documents \n",
                "    according to your embedder.\n",
                "\n",
                "    :param golden_matches: A dictionary with the gold standard matches, where the key is the query ID \n",
                "                           and the value is the ID of the correct document.\n",
                "    :param results: A dictionary with the search results, where the key is the query ID and the value \n",
                "                    is a dictionary of document IDs and their similarity scores to the query.\n",
                "    :return: The value of the nDCG metric.\n",
                "    \"\"\"\n",
                "\n",
                "    for query_id, v in results.items():\n",
                "        results[query_id] = {k: v for k, v in sorted(v.items(), key=lambda item: -item[1])}\n",
                "\n",
                "    ndcg_sum = 0\n",
                "    for query_id, v in results.items():\n",
                "        golden_document = golden_matches[query_id]\n",
                "        for i, document_id in enumerate(v.keys()):\n",
                "            if golden_document == document_id:\n",
                "                ndcg_sum += 1 / log2(i + 2)\n",
                "\n",
                "    ndcg = round(ndcg_sum / len(results), 5)\n",
                "    return ndcg\n",
                "\n",
                "\n",
                "def compute_score(ndcg: float) -> float:\n",
                "    \"\"\"\n",
                "    Computes the final score based on the value of the nDCG metric.\n",
                "    \"\"\"\n",
                "    lower_bound = 0.2\n",
                "    upper_bound = 0.5\n",
                "\n",
                "    if ndcg <= lower_bound:\n",
                "        return 0\n",
                "    elif lower_bound < ndcg < upper_bound:\n",
                "        return int(round(100 * (ndcg - lower_bound) / (upper_bound - lower_bound)))\n",
                "    else:\n",
                "        return 100"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "6011bdd3",
            "metadata": {},
            "source": [
                "### Retrieval  \n",
                "Below is the code used to select the top_k best documents from the corpus for a given query."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "deb8158e",
            "metadata": {},
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "def cos_sim(a: torch.Tensor, b: torch.Tensor):\n",
                "    \"\"\"\n",
                "    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
                "    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
                "    \"\"\"\n",
                "    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
                "    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
                "    return torch.mm(a_norm, b_norm.transpose(0, 1))\n",
                "\n",
                "def search_topk_texts(\n",
                "    embedder,\n",
                "    corpus: dict[str, dict[str, str]],\n",
                "    queries: dict[str, str],\n",
                "    top_k: int = 10,\n",
                ") -> dict[str, dict[str, float]]:\n",
                "    results = {}\n",
                "\n",
                "    # Create embeddings for all queries using model.encode_queries()\n",
                "    # Runs semantic search against the corpus embeddings\n",
                "    # Returns a ranked list with the corpus ids\n",
                "    query_ids = list(queries.keys())\n",
                "    results = {qid: {} for qid in query_ids}\n",
                "    queries = [queries[qid] for qid in queries]\n",
                "    query_embeddings = embedder.encode_queries(queries)\n",
                "\n",
                "    corpus_ids = sorted(\n",
                "        corpus,\n",
                "        key=lambda k: len(corpus[k].get(\"title\", \"\") + corpus[k].get(\"text\", \"\")),\n",
                "        reverse=True,\n",
                "    )\n",
                "    corpus = [corpus[cid] for cid in corpus_ids]\n",
                "\n",
                "    # Encode chunk of corpus\n",
                "    corpus_embeddings = embedder.encode_corpus(corpus)\n",
                "\n",
                "    # Compute similarites using cosine-similarity\n",
                "    cos_scores = cos_sim(query_embeddings, corpus_embeddings)\n",
                "    cos_scores[torch.isnan(cos_scores)] = -1\n",
                "\n",
                "    # Get top-k values\n",
                "    cos_scores_top_k_values, cos_scores_top_k_idx = torch.topk(\n",
                "        cos_scores,\n",
                "        min(top_k + 1, len(cos_scores[1])),\n",
                "        dim=1,\n",
                "        largest=True,\n",
                "        sorted=False,\n",
                "    )\n",
                "    cos_scores_top_k_values = cos_scores_top_k_values.cpu().tolist()\n",
                "    cos_scores_top_k_idx = cos_scores_top_k_idx.cpu().tolist()\n",
                "\n",
                "    for query_itr in range(len(query_embeddings)):\n",
                "        query_id = query_ids[query_itr]\n",
                "        for score, corpus_id in zip(cos_scores_top_k_values[query_itr], cos_scores_top_k_idx[query_itr]):\n",
                "            results[query_id][corpus_ids[corpus_id]] = score\n",
                "\n",
                "    return results"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "894752b3",
            "metadata": {},
            "source": [
                "# Your Solution  \n",
                "Place your solution in this section. Make changes only here!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4cd73aba",
            "metadata": {},
            "outputs": [],
            "source": [
                "class Embedder:\n",
                "    # Do not change the constructor signature\n",
                "    def __init__(self):\n",
                "        # TODO: you can modify this method,\n",
                "        # but do not change its signature! (i.e., do not change the arguments)\n",
                "        self.model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
                "        self.tokenizer = Tokenizer(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
                "\n",
                "    def encode_queries(self, queries: list[str]):\n",
                "        \"\"\"\n",
                "        Function for encoding queries.\n",
                "        :param queries: A list of queries to be encoded.\n",
                "        :return: Query embeddings - a tensor of shape (n, 768), where n = len(queries) is the number of queries.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: implement this method – encode the queries\n",
                "        # Do not change the signature of this method! (i.e., do not change the arguments)\n",
                "        # Remember, you can use the HuggingFace gpt-2 model...\n",
                "        # You may use the Tokenizer implemented at the top of the notebook\n",
                "        # Hint: Evaluation will be faster if the returned tensor is on the GPU.\n",
                "        ...\n",
                "        return torch.ones(len(queries), 768).to(device)\n",
                "\n",
                "    def encode_corpus(self, texts: list[dict]):\n",
                "        \"\"\"\n",
                "        Function for encoding source texts.\n",
                "        :param texts: A list of texts to be encoded. Each text is represented as a dictionary:\n",
                "            {\n",
                "                \"title\": \"...\"\n",
                "                \"text\": \"...\",\n",
                "            }\n",
                "        :return: Text embeddings - a tensor of shape (m, 768), where m = len(texts) is the number of texts.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: implement this method – encode the source texts\n",
                "        # Do not change the signature of this method! (i.e., do not change the arguments)\n",
                "        ...\n",
                "        return torch.ones(len(texts), 768).to(device)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3966b49e",
            "metadata": {},
            "source": [
                "# Evaluation\n",
                "\n",
                "Running the cell below will allow you to check how many points your solution would score on the validation data.  \n",
                "Before submitting, make sure the entire notebook runs from start to finish without errors and without requiring any user intervention after selecting \"Run All\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea9f4204",
            "metadata": {},
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "if not FINAL_EVALUATION_MODE:\n",
                "    embedder = Embedder()\n",
                "\n",
                "    with torch.no_grad():\n",
                "        results = search_topk_texts(embedder, corpus, queries, top_k=10)\n",
                "\n",
                "    # Obliczenie nDCG\n",
                "    ndcg = evaluate_retrieval_ndcg(matching_texts, results)\n",
                "\n",
                "    # Obliczenie końcowego wyniku na podstawie nDCG\n",
                "    points = compute_score(ndcg)\n",
                "\n",
                "    print(f\"\\nLiczba zapytań: {len(queries)}\")\n",
                "    print(f\"Liczba tekstów: {len(corpus)}\")\n",
                "    print(f\"nDCG: {ndcg:.3f}\")\n",
                "    print(f\"Wynik punktowy: {points}\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "91ee3bc2",
            "metadata": {},
            "source": [
                "During evaluation, the model will be saved as `your_model.pkl` and evaluated on the test set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "40bd5444",
            "metadata": {},
            "outputs": [],
            "source": [
                "######################### DO NOT MODIFY THIS CELL ##########################\n",
                "\n",
                "if not FINAL_EVALUATION_MODE:\n",
                "    embedder = Embedder()\n",
                "\n",
                "    with torch.no_grad():\n",
                "        results = search_topk_texts(embedder, corpus, queries, top_k=10)\n",
                "\n",
                "    # Compute nDCG\n",
                "    ndcg = evaluate_retrieval_ndcg(matching_texts, results)\n",
                "\n",
                "    # Compute final score based on nDCG\n",
                "    points = compute_score(ndcg)\n",
                "\n",
                "    print(f\"\\nNumber of queries: {len(queries)}\")\n",
                "    print(f\"Number of texts: {len(corpus)}\")\n",
                "    print(f\"nDCG: {ndcg:.3f}\")\n",
                "    print(f\"Score: {points}\")"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
