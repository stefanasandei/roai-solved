{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.random.manual_seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "test_split = 0.33\n",
    "batch_size = 4\n",
    "epochs = 4\n",
    "\n",
    "root_path = \"E:\\\\IOAI\\\\kits\\\\nitro-nlp-24\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, csv_path):\n",
    "        self.df = pd.read_csv(f\"{root_path}\\\\{csv_path}\")\n",
    "\n",
    "        if not \"test\" in csv_path:\n",
    "            self.df = self.df.sample(frac=0.1, random_state=seed)\n",
    "\n",
    "        self.df[[\"title\", \"content\"]] = self.df[[\"title\", \"content\"]].fillna(value=\"\")\n",
    "        self.df[\"text\"] = self.df[\"title\"]# + \" \" + self.df[\"content\"]\n",
    "        self.df[\"text\"] = self.df[\"text\"].apply(self.clean_text)\n",
    "\n",
    "        if \"class\" in self.df:\n",
    "            self.df[\"class\"] = self.df[\"class\"].astype(np.uint8)\n",
    "\n",
    "        # self.max_length = max(self.df[\"text\"].apply(lambda s: len(s)))\n",
    "        self.max_length = 64\n",
    "\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\n",
    "            \"dumitrescustefan/bert-base-romanian-cased-v1\",\n",
    "            strip_accents=False, lower_case=True\n",
    "        )\n",
    "\n",
    "    def clean_text(self, text: str):\n",
    "        text = re.sub(r\"http[s]?://\\S+|www\\.\\S+\", \" \", text)\n",
    "        text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "        text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
    "        text = re.sub(r\"[\\$€£¥₹]\", \" moneda \", text)\n",
    "        text = str(unicodedata.normalize('NFC', text).encode(\"ascii\", \"ignore\"))\n",
    "        text = re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "        def _reduce_repeats(match):\n",
    "            char = match.group(1)\n",
    "            return char * 2\n",
    "\n",
    "        text = re.sub(r\"(\\w)\\1{2,}\", _reduce_repeats, text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\s{2,}\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            row[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True, \n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False, \n",
    "        )\n",
    "\n",
    "        if \"class\" in self.df:\n",
    "            return {\n",
    "                'id': row[\"id\"],\n",
    "                'input_ids': encoding['input_ids'].squeeze(0), # (seq_len)\n",
    "                'attention_mask': encoding['attention_mask'].squeeze(0), # (seq_len)\n",
    "                'label': torch.tensor(row[\"class\"], dtype=torch.long) # (1)\n",
    "            }\n",
    "        return {\n",
    "            \"id\": row[\"id\"],\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),  # (seq_len)\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),  # (seq_len)\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1183, 583)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextDataset(\"train.csv\")\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "test_size = int(test_split * dataset_size)\n",
    "train_size = dataset_size - test_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([4]), torch.Size([4, 64]), torch.Size([4, 64]), torch.Size([4])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "[v.shape for v in batch.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dumitrescustefan/bert-base-romanian-cased-v1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"dumitrescustefan/bert-base-romanian-cased-v1\", num_labels=2)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8526, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = batch[\"input_ids\"].to(device)\n",
    "attention_mask = batch[\"attention_mask\"].to(device)\n",
    "labels = batch[\"label\"].to(device)\n",
    "\n",
    "outputs = model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")\n",
    "\n",
    "outputs.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    balanced_accuracy = torchmetrics.Accuracy(\n",
    "        task=\"multiclass\", num_classes=2, average=\"macro\"\n",
    "    ).to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "\n",
    "            preds = outputs.logits.argmax(dim=-1)\n",
    "            balanced_accuracy.update(preds, labels)\n",
    "\n",
    "    return balanced_accuracy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1183/1183 [01:28<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1; loss=0.267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1183/1183 [01:30<00:00, 13.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2; loss=0.116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1183/1183 [01:28<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3; loss=0.038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1183/1183 [01:28<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4; loss=0.016\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_train:\n\u001b[0;32m     32\u001b[0m     train()\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m \u001b[38;5;66;03m# to pause notebook execution\u001b[39;00m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch in tqdm(train_loader):\n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"].to(device),\n",
    "                batch[\"attention_mask\"].to(device),\n",
    "                batch[\"label\"].to(device),\n",
    "            )\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # backward pass\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch}; loss={(running_loss/len(train_loader)):.3f}\")\n",
    "    torch.save(model.state_dict(), \"bert_sarcasm.pth\")\n",
    "\n",
    "should_train = True\n",
    "if should_train:\n",
    "    train()\n",
    "    # 1/0 # to pause notebook execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 583/583 [00:10<00:00, 57.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9224, device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert_sarcasm.pth\"))\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36669"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TextDataset(\"test.csv\")\n",
    "val_loader = DataLoader(dataset, batch_size=1)\n",
    "len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36669/36669 [06:49<00:00, 89.48it/s]\n"
     ]
    }
   ],
   "source": [
    "subtask, ids = [], []\n",
    "\n",
    "for batch in tqdm(val_loader):\n",
    "    input_ids, attention_mask = (\n",
    "        batch[\"input_ids\"].to(device),\n",
    "        batch[\"attention_mask\"].to(device),\n",
    "    )\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "\n",
    "    pred = torch.argmax(outputs.logits).item()\n",
    "\n",
    "    subtask.append(pred)\n",
    "    ids.append(batch[\"id\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    25766\n",
       "1    10903\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame({\"id\": ids, \"class\": subtask})\n",
    "\n",
    "submission[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
